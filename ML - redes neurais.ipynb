{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WS7JLm2DGZ_d",
        "QNwSxaqiGyoZ",
        "DK5gDZLL982c",
        "VCqNeUy0Kxck"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REDES NEURAIS\n",
        "\n",
        "O objetivo desse *notebook* é apresentar os fundamentos matemáticos que baseiam as redes neurais. Para isso, nós iremos implementar uma rede neural e usar o Método de Newton para encontrar uma aproximação numérica para os parâmetros que melhor ajustam uma rede neural a um certo conjunto de dados.\n"
      ],
      "metadata": {
        "id": "WS7JLm2DGZ_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma *rede neural* é uma função $f : X \\to Y$ que geralmente é *ajustada* (ou *treinada*) para modelar a relação entrada-saída de um conjunto de dados. Nesse caso, o domínio da rede neural (o conjunto $X$) deve conter os vetores-entrada, o contradomínio (o conjunto $Y$) deve conter os vetores-saída desse conjunto de dados, e os valores da rede neural ajustada aplicada aos vetores-entrada devem estar o mais próximo possível dos respectivos vetores-saída.\n"
      ],
      "metadata": {
        "id": "uaSw8uwK8Uc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Existem diversos tipos de redes neurais. O mais simples é chamado de *perceptron* (ou, em geral, *multi-layer perceptron*). Nesse caso mais simples, a função $f$ é uma composição de várias funções,\n",
        "$$\n",
        "    f = f_n \\circ f_{n-1} \\circ \\dotsb \\circ f_2 \\circ f_1,\n",
        "$$\n",
        "onde cada uma das funções $f_1, f_2, \\dotsc, f_n$ é chamada de *camada de $f$*. Além disso, cada função $f_i$ também pode ser decomposta como uma composição: de uma multiplicação por uma matriz (chamada de *pesos*), com a soma de um vetor (chamado de *viéses*), com a aplicação de uma *função de ativação*. Nas próximas seções, nós veremos como isso funciona em mais detalhes.\n"
      ],
      "metadata": {
        "id": "3fHjs39aTluS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rede neural\n",
        "\n",
        "Nessa seção, vamos implementar uma rede neural. Para isso, precisaremos importar algumas bibliotecas, escolher as camadas da rede neural e escolher a função de ativação que será usada. A partir dessas informações, definiremos os parâmetros da rede e a função *rede neural*.\n"
      ],
      "metadata": {
        "id": "QNwSxaqiGyoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pacotes e bibliotecas\n",
        "\n",
        "Vamos começar importando um pacote e uma biblioteca de Python: numpy e sympy. O primeiro será usada para cálculos numéricos e a segunda será usada para realizar manipulações simbólicas.\n"
      ],
      "metadata": {
        "id": "1-XNKXZFGulM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaYJNn6yk50h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sympy as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensões das camadas\n",
        "\n",
        "Agora vamos definir as dimensões das camadas da rede neural, ou seja, vamos escolher uma lista de números naturais $(d_0, d_1, \\dotsc, d_n)$, onde $d_0$ é a dimensão da camada de entrada, $d_1$ é a dimensão da primeira camada oculta, $\\dotsc$ , e $d_n$ é a dimensão da camada de saída. Observe que o caso mais simples possível é aquele em que $d_0 = d_1 = 1$. Nesse caso, temos 1 neurônio na camada de entrada e 1 neurônio na camada de saída.\n"
      ],
      "metadata": {
        "id": "yimt0DfP_KTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir dimensões das camadas:\n",
        "\n",
        "dimensoes_camadas = [1, 1]"
      ],
      "metadata": {
        "id": "IwuvrcnU_UvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de prosseguirmos, vamos verificar se a lista acima contem pelo menos duas camadas e se a dimensão de cada camada é um número inteiro maior que zero. Se a validação funcionar, vamos exibir uma ilustração da rede neural que será construída.\n"
      ],
      "metadata": {
        "id": "_E3EBtABYnIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validação da lista de dimensões\n",
        "\n",
        "if len(dimensoes_camadas) < 2:\n",
        "    raise ValueError(\"A rede neural precisa de pelo menos 2 camadas.\")\n",
        "else:\n",
        "    for dim in dimensoes_camadas:\n",
        "        if not isinstance(dim, int):\n",
        "            raise ValueError(f\"A dimensão de uma das camadas não é um número inteiro: {dim}.\")\n",
        "        elif dim < 1:\n",
        "            raise ValueError(f\"A dimensão de uma das camadas não é maior que zero: {dim}.\")\n",
        "\n",
        "\n",
        "# Visualização da rede neural\n",
        "\n",
        "!pip install graphviz -qq\n",
        "import graphviz\n",
        "\n",
        "grafico = graphviz.Digraph(comment='Rede Neural', graph_attr={'rankdir': 'LR'})\n",
        "\n",
        "nodes_by_layer = []\n",
        "for k in range(len(dimensoes_camadas)):\n",
        "    layer_nodes = []\n",
        "    with grafico.subgraph(name=f'rank_{k}') as s:\n",
        "        s.attr(rank='same')\n",
        "        if k == 0:\n",
        "            layer_nodes = [f'input_{i}' for i in range(dimensoes_camadas[k])]\n",
        "            node_color = 'lightblue'\n",
        "        elif k == len(dimensoes_camadas) - 1:\n",
        "            layer_nodes = [f'output_{i}' for i in range(dimensoes_camadas[k])]\n",
        "            node_color = 'lightcoral'\n",
        "        else:\n",
        "            layer_nodes = [f'hidden_{k}_{i}' for i in range(dimensoes_camadas[k])]\n",
        "            node_color = 'lightgray'\n",
        "\n",
        "        nodes_by_layer.append(layer_nodes)\n",
        "\n",
        "        for node_name in layer_nodes:\n",
        "             s.node(node_name, shape='circle', label='', width='0.2', height='0.2', color=node_color, style='filled')\n",
        "\n",
        "for k in range(len(dimensoes_camadas) - 1):\n",
        "    current_layer_nodes = nodes_by_layer[k]\n",
        "    next_layer_nodes = nodes_by_layer[k+1]\n",
        "\n",
        "    for source_node in current_layer_nodes:\n",
        "        for target_node in next_layer_nodes:\n",
        "            grafico.edge(source_node, target_node, arrowhead='none', color='gray')\n",
        "\n",
        "display(grafico)"
      ],
      "metadata": {
        "id": "flH913xxYyHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parâmetros\n",
        "\n",
        "Com as dimensões das camadas da rede neural escolhidas, nós podemos definir os parâmetros da rede neural. Para cada $k \\in \\{0, 1, \\dotsc, n-1\\}$, as variáveis $w_{kij}$ representam as entradas da matriz de pesos $w_k = \\left( {w_k}_{ij} \\right)_{ij}$ e as variáveis $b_{kj}$ representam as coordenadas do vetor de viés $b_k = ({b_k}_1, \\dotsc, {b_k}_{d_k})$ da $k$-ésima camada. Por exemplo, no caso mais simples (em que $d_0 = d_1 = 1$), os parâmetros são apenas $w_{000}$ e $b_{00}$.\n"
      ],
      "metadata": {
        "id": "xlHkEGYM_QZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros da rede neural\n",
        "\n",
        "profundidade = len(dimensoes_camadas)-1\n",
        "\n",
        "## w[k][i][j] é o peso do link j -> i na camada k\n",
        "w = [[[sp.Symbol(f\"w{k}{i}{j}\") for j in range(dimensoes_camadas[k])]  # Corrigido: Symbol com S maiúsculo\n",
        "      for i in range(dimensoes_camadas[k+1])]\n",
        "      for k in range(profundidade)]\n",
        "\n",
        "## b[k][j] é o viés somado à coordenada j na camada k\n",
        "b = [[sp.Symbol(f\"b{k}{j}\") for j in range(dimensoes_camadas[k+1])]\n",
        "     for k in range(profundidade)]\n",
        "\n",
        "## listona que contem todos os w's e todos os b's\n",
        "parametros = [*np.array(w).flatten(), *np.array(b).flatten()]"
      ],
      "metadata": {
        "id": "GZCG1IMWf9aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função de ativação\n",
        "\n",
        "Agora vamos escolher a função de ativação que será usada na rede neural. Em um caso geral, nós podemos escolher uma função para cada par de neurônios. Mas nesse caso, por simplicidade, nós vamos escolher uma única função de ativação para todas as conexões da nossa rede neural. Mais especificamente, vamos escolher a função *sigmóide*, que é a função $\\sigma: \\mathbb R \\to \\mathbb R$ definida por $\\sigma(x) = \\frac{1}{1+e^{-x}}$. Também é possível definir outras funções (como *ReLU*, *tanh*, etc.) nessa célula.\n"
      ],
      "metadata": {
        "id": "tdCpGLJp_GpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir a função de ativação:\n",
        "\n",
        "x = sp.Symbol('x')\n",
        "sigma = 1/(1 + sp.exp(-x))"
      ],
      "metadata": {
        "id": "Jdwctdmvmhxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rede neural\n",
        "\n",
        "Agora vamos definir a rede neural *per se*.  Por definição, para cada camada $k \\in \\{0, 1, \\dotsc, n-1\\}$, a rede multiplica o vetor que sai da camada anterior pela matriz de pesos $w_k$, soma o resultado com o vetor de vieses $b_k$, e aplica a função de ativação $\\sigma$ a cada coordenada do vetor resultante.\n"
      ],
      "metadata": {
        "id": "d1wGkbVq_bOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que avalia a rede neural em um vetor\n",
        "\n",
        "def rede_neural (entrada):\n",
        "    vetor = entrada\n",
        "    for k in range(profundidade):\n",
        "        vetor = [sum([w[k][i][j] * vetor[j] for j in range(dimensoes_camadas[k])]) + b[k][i]\n",
        "                 for i in range(dimensoes_camadas[k+1])]\n",
        "        vetor = [sigma.subs(x, vetor[i]) for i in range(dimensoes_camadas[k+1])]\n",
        "    return vetor"
      ],
      "metadata": {
        "id": "cnv5UNW1qqN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dados\n",
        "\n",
        "Lembre que uma rede neural pode ser ajustada a um conjunto de dados. Nessa parte, vamos definir um tal conjunto, formatá-lo adequadamente e realizar verificações para garantir sua consistência com a arquitetura da rede neural definida acima.\n"
      ],
      "metadata": {
        "id": "DK5gDZLL982c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para começar, considere um conjunto de pares da forma\n",
        "$$\n",
        "    D = \\{(x_1, y_1), (x_2, y_2), \\dotsc, (x_m, y_m)\\},\n",
        "$$\n",
        "onde $m > 0$ é a quantidade de *samples*, $x_1, x_2, \\dotsc, x_m \\in \\mathbb{R}^{d_0}$ são vetores-entrada, e $y_1, y_2, \\dotsc, y_m \\in \\mathbb{R}^{d_n}$ são vetores-saída. O objetivo do ajuste (ou treinamento) da rede neural é encontrar parâmetros (pesos e vieses) que façam com que a imagem do vetor $x_i$ pela rede neural seja o mais próximo possível do vetor $y_i$, para todo $i \\in \\{1, 2, \\dotsc, m\\}$.\n"
      ],
      "metadata": {
        "id": "7BgJnQajJLtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir os conjunto de dados aos quais a rede será ajustada:\n",
        "\n",
        "dados = {(0, 0), (1, 1), (2, 0), (3, 1)}"
      ],
      "metadata": {
        "id": "DzzxXyVM-Afv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de continuar, vamos tratar e validar o conjunto de dados acima. Para começar, vamos verificar que cada um dos dados é formado por duas partes, uma que representa um vetor-entrada e a outra que representa um vetor-saída.\n"
      ],
      "metadata": {
        "id": "hIJVqtw21pML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificação de que todos os dados têm duas partes\n",
        "\n",
        "for dado in dados:\n",
        "    if len(dado) != 2:\n",
        "        raise ValueError(f\"Erro nos dados: o ponto {dado} tem {len(dado)} partes, mas deveria ter 2 partes.\")"
      ],
      "metadata": {
        "id": "9HXp0k9vknGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se alguma mensagem tiver sido impressa na saída da célula acima, nós não poderemos continuar. Caso contrário, vamos formatar os dados como pares de tuplas, uma que representa um vetor-entrada e a outra que representa um vetor-saída.\n"
      ],
      "metadata": {
        "id": "uyQASF0ilJLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatação do conjunto de dados\n",
        "\n",
        "dados_formatados = []\n",
        "\n",
        "for dado in dados:\n",
        "\n",
        "    dado_entrada = dado[0]\n",
        "    if isinstance(dado_entrada, (int, float)):\n",
        "        dado_entrada = (dado_entrada,)\n",
        "    elif isinstance(dado_entrada, (tuple, list)):\n",
        "        dado_entrada = tuple(dado_entrada)\n",
        "    else:\n",
        "        raise ValueError(f\"Erro na formatação dos dados: {dado} é de tipo {type(dado)}.\")\n",
        "\n",
        "    dado_saida = dado[1]\n",
        "    if isinstance(dado_saida, (int, float)):\n",
        "        dado_saida = (dado_saida,)\n",
        "    elif isinstance(dado_saida, (tuple, list)):\n",
        "        dado_saida = tuple(dado_saida)\n",
        "    else:\n",
        "        raise ValueError(f\"Erro na formatação dos dados: {dado} é de tipo {type(dado)}.\")\n",
        "\n",
        "    dado = (dado_entrada, dado_saida)\n",
        "    dados_formatados.append(dado)\n",
        "\n",
        "dados = dados_formatados"
      ],
      "metadata": {
        "id": "zpk0NbJbGzG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se alguma mensagem tiver sido impressa na saída da célula acima, nós não poderemos continuar. Caso contrário, vamos verificar se as dimensões dos vetores-entrada de todos os dados estão coincidindo com a dimensão da camadas de entrada da rede, e analogamente, se as dimensões dos vetores-saída de todos os dados estão coincidindo com a dimensão da camada de saída da rede.\n"
      ],
      "metadata": {
        "id": "FVfAkVnP_rGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificação da consistência das dimensões\n",
        "\n",
        "for dado in dados:\n",
        "    if len(dado[0]) != dimensoes_camadas[0] or len(dado[1]) != dimensoes_camadas[-1]:  # Corrigido\n",
        "        raise ValueError(f\"A dimensão do dado {dado} não está consistente com a arquitetura da rede.\")"
      ],
      "metadata": {
        "id": "ReH-I5vuEPm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajuste da rede neural\n",
        "\n",
        "Lembre que uma das características mais importantes das redes neurais para aplicações é que elas são suficientemente flexíveis para se ajustar bem a diversos conjuntos de dados. Como consequência, isso permite que elas sejam úteis para modelar diversos fenômenos. Nessa parte, vamos calcular os parâmetros da rede que fazem com que ela se ajuste melhor ao conjunto de dados fornecido acima. Para isso, nós definiremos uma função erro e utilizaremos o Método de Newton para encontrar uma aproximação para os valores dos parâmetros que minimizam esse erro.\n"
      ],
      "metadata": {
        "id": "VCqNeUy0Kxck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Função erro (*loss function*)\n",
        "\n",
        "A função erro é obtida comparando as imagens da rede neural nos vetores-entrada do conjunto de dados com os seus respectivos vetores-saída. Essa função pode ser definida de diversas maneiras. Nós vamos defini-la como a soma dos quadrados das diferenças entre esses dois vetores.\n"
      ],
      "metadata": {
        "id": "eBfpJ0yB2cGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que calcula o erro entre a imagem da rede e os dados de entrada\n",
        "\n",
        "def erro (dados):\n",
        "    saida = [rede_neural(dado[0]) for dado in dados]\n",
        "    dados_saida = [dado[1] for dado in dados]\n",
        "    L = sum([(saida[i][j] - dados_saida[i][j])**2 for i in range(len(dados)) for j in range(dimensoes_camadas[-1])])\n",
        "    return L"
      ],
      "metadata": {
        "id": "XHW1KPpP9uGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimização da função erro\n",
        "\n",
        "Lembre que ajustar uma rede neural consiste em encontrar os parâmetros dessa rede que minimizam a função erro $L$. Como essa função depende de várias variáveis (os parâmetros da rede), lembre da disciplina de *Cálculo em Várias Variáveis* que os seus pontos de mínimo anulam seu gradiente. Depois observe que a equação $\\nabla L = (0, \\dotsc, 0)$ é equivalente a um sistema de equações que não é necessariamente linear. Um dos métodos numéricos mais usados na resolução desse tipo de sistema é o chamado *Método de Newton*. Esse é um método iterativo que constrói uma sequência de aproximações para uma solução do sistema de equações desejado.\n",
        "<!--Para isso, nós começamos com uma aproximação inicial (que, nesse caso, nós vamos escolher como $Z_0 = (0, \\dotsc, 0)$) e definimos a próxima aproximação como a solução do sistema linear\n",
        "$$\n",
        "    J(Z_n) X = J(Z_n) Z_n - \\nabla L(Z_n),\n",
        "$$\n",
        "onde $J(Z_n)$ é a matriz Jacobiana de $\\nabla L$ (ou, equivalentemente, a matriz Hessiana de $L$) avaliada na $n$-ésima aproximação $Z_n$.-->\n"
      ],
      "metadata": {
        "id": "BeiDuMJP2mlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de começarmos a aplicar o Método de Newton à rede neural construida acima, vamos estabelecer alguns *critérios de parada* para ele; ou seja, vamos estabelecer quando nós consideraremos que uma aproximação para a solução da equação $\\nabla L = (0, \\dotsc, 0)$ será boa. Existem vários critérios de parada possíveis. Nós escolheremos o seguinte.\n",
        "\n",
        "Dados $\\delta > 0$, $\\varepsilon > 0$ e $M > 0$, nós consideraremos que uma aproximação para a solução da equação $\\nabla L = (0, \\dotsc, 0)$ é boa quando:\n",
        "\n",
        " * as aproximações não estiverem mudando muito, ou seja, $\\|Z_{i+1} - Z_i\\| < \\delta$,\n",
        "\n",
        " * ou o gradiente estiver próximo de zero, ou seja, $\\| \\nabla L(Z_i)\\| < \\varepsilon$,\n",
        "\n",
        " * ou o número de iterações já for muito grande, ou seja, $i > M$.\n",
        "\n",
        "Agora vamos escolher valores concretos para $\\delta$, $\\varepsilon$ e $M$.\n"
      ],
      "metadata": {
        "id": "qqnfPBE_2xyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros dos critérios de parada:\n",
        "\n",
        "delta = 10**(-5)\n",
        "epsilon = 10**(-5)\n",
        "max_iteracoes = 100"
      ],
      "metadata": {
        "id": "aaGrH0ys2ofp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para concluir, vamos implementar o Método de Newton.\n"
      ],
      "metadata": {
        "id": "GYB0bGdQq_3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo da função erro\n",
        "L = erro(dados)\n",
        "\n",
        "# Gradiente da função erro\n",
        "gradL = [sp.diff(L, p) for p in parametros]\n",
        "\n",
        "# Aproximação inicial\n",
        "z = np.zeros(len(parametros), dtype=np.float32)\n",
        "\n",
        "# Lista que vai guardar as aproximações do Método de Newton\n",
        "Z = []\n",
        "Z.append(z)"
      ],
      "metadata": {
        "id": "WdE5xGcRGhH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que calcula a matriz Jacobiana\n",
        "\n",
        "def jacobiana (funcao, vetor):\n",
        "    n = len(parametros)\n",
        "\n",
        "    if len(funcao) != n:\n",
        "        raise ValueError(f\"A função dada tem {len(funcao)} entradas, mas deveria ter {n} entradas.\")\n",
        "\n",
        "    elif len(vetor) != n:\n",
        "        raise ValueError(f\"O vetor dado tem {len(vetor)} entradas, mas deveria ter {n} entradas.\")\n",
        "\n",
        "    else:\n",
        "        matriz = np.zeros([n, n])\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                fi = funcao[i]\n",
        "                xj = parametros[j]\n",
        "                dfidxj = sp.diff(fi, xj)\n",
        "                matriz[i][j] = sp.lambdify(parametros, dfidxj)(*vetor)\n",
        "\n",
        "        return matriz"
      ],
      "metadata": {
        "id": "hoMijKoA2arf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função que calcula o lado direito da equação J(Z) X = J(Z)Z - ∇L(Z)\n",
        "\n",
        "def lado_direito (funcao, vetor):\n",
        "    if len(vetor) != len(parametros):\n",
        "        raise ValueError(f\"O vetor {vetor} tem {len(vetor)} entradas, mas deveria ter {len(parametros)} entradas.\")\n",
        "    else:\n",
        "        DFZZ = jacobiana(funcao, vetor) @ vetor\n",
        "        F = sp.lambdify(parametros, funcao)\n",
        "        FZ = F(*vetor)\n",
        "        saida = DFZZ - FZ\n",
        "        return np.array(saida, dtype=np.float32)"
      ],
      "metadata": {
        "id": "bGP2dbtXV29V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop completo do Método de Newton\n",
        "\n",
        "F = sp.lambdify(parametros, gradL)\n",
        "\n",
        "A = jacobiana(gradL, Z[-1])\n",
        "B = lado_direito(gradL, Z[-1])\n",
        "z = np.linalg.solve(A, B)\n",
        "Z.append(z)\n",
        "\n",
        "while (np.linalg.norm(Z[-1] - Z[-2], ord=1) > delta and\n",
        "       np.linalg.norm(F(*Z[-1]), ord=1) > epsilon and\n",
        "       len(Z) < max_iteracoes):\n",
        "\n",
        "    A = jacobiana(gradL, Z[-1])\n",
        "    B = lado_direito(gradL, Z[-1])\n",
        "    z = np.linalg.solve(A, B)\n",
        "    Z.append(z)\n",
        "\n",
        "for i in range(len(Z)):\n",
        "    print(f\"Z_{i} = {Z[i]}\")"
      ],
      "metadata": {
        "id": "wGcXmEeDq2oU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}